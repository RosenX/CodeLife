# CART算法

分类回归决策树(classification and regression tree, CART)同样由特征选择、决策树生成与决策树剪枝这三个步骤组成，可用于回归和分类

CART生成的决策树是二叉树，每一次根据一个特征进行二分，最终将整个特征空间分成有限个区域，每个区域输出一个预测值

## 决策树生成

### 回归树生成

假设最终分割的区域为：$R_1, R_2,...,R_M$，每个区域有固定的输出值$c_m$，则回归树模型可以表示为：
$$
f(x) = \sum_{m=1}^{M}c_mI(x \in R_m)
$$

回归树采用平方误差度量损失, 每一次选择特征的优化目标可以表示为：
$$
\min_{j,s} \left[ \min_{c_1} \sum_{x_i \in R_1(j,s)} (y_i-c_1)^2 +  \min_{c_1} \sum_{x_i \in R_2(j,s)} (y_i-c_1)^2 \right]
$$

其中$R_1(j,s) = \{x| x^{j} \le s\}$, $R_2(j,s) = \{x| x^{j} > s\}$

求解方法：遍历每个特征$x^j$，遍历其中每个分割点$s$，$c_m = avg(y_i | x_i \in R_m)$，计算上式loss。最终取loss最小的j和s。

通过以上准则递归得对数据进行分割，直到满足停止条件为止。

### 分类树生成

分类树采用基尼系数选择最优特征:
$$
Gini(p) = \sum_{k=1}^K p_k(1-p_k) = 1-\sum_{k=1}^Kp_k^2 \\

Gini(D) = 1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2 \\

Gini(D,A) = \frac{|D_1|}{|D|} Gini(D_1) + \frac{|D_2|}{|D|} Gini(D_2)
$$

基尼系数同样表示集合的不确定性，对于给你特征集A和数据集D，选择使$Gini(D,A_g=a)$**最小**的特征$A_g=a$作为分割点即可。


通过以上准则递归得对数据进行分割，直到满足停止条件为止。


### 停止条件

可以有多种停止条件，如：
1. 损失小于e，全部为同一类样本
2. 节点中样本数量少于一定个数
3. 树的深度达到阈值


## 决策树剪枝

整体思路：假设原始决策树为$T_0$，可以通过不断的裁剪子树知道只剩下根节点，得到一个树序列$T_0, T_1, T_2,..., T_n$，然后通过数据集进行交叉验证选择最优子树。

假设损失函数为:
$$
C_\alpha(T) = C(T) + \alpha|T|
$$
其中$C(T)$为误差（均方差，基尼指数），$|T|$为叶子节点个数，$\alpha$为权衡参数。

当$\alpha$越大，树的节点越少。取不同的$\alpha$可以得到不同的子树。即对于$0=\alpha_0<\alpha_1<...<\alpha_n<+\infty$，区间$[\alpha_i,\alpha_{i+1}), i = 0,1,2,3,...,n$分别对应着子树$T_0, T_1, T_2,..., T_n$。因此可以通过求出不同的$\alpha$确定不同的子树。

设t为某一内部节点，当要裁剪以$t$为根的子树$T_{t}$时，需要满足:
$$
C_{\alpha}(t) = C(t)+\alpha > C(T_t)+\alpha|T_t| = C_{\alpha}(T_t)
$$

临界点如下，因此可得一系列区间
$$
\alpha = \frac{C(t)-C(T_t)}{|T_t|-1}
$$

具体剪枝算法
> 输入：$T_0$ \
> 输出：$T_{\alpha}$ \
> 步骤：
> 1. 设$k = 0, T = T_0$
> 2. 设$\alpha = +\infty$
> 3. 自下而上对各内部节点计算$C(T_{t}), |T_t|$，以及
>  $$
    g(t) = \frac{C(t)-C(T_t)}{|T_t|-1} \\
    \alpha = min(\alpha, g(t))
>  $$
> 4. 对$g(t) = \alpha$的内部节点进行剪枝， 并将t节点内的样本类别设置为多数类，得到数T
> 5. 设$k = k+1, T_k = T, \alpha_k = \alpha$
> 6. 如果只剩下根节点及两个叶子节点，设$T_k = T_n$。否则，回到步骤2
> 7. 用交叉验证法从$T_0, T_1, T_2, ..., T_n$中选择$T_{\alpha}$


## 参考

1. 李航《机器学习方法》