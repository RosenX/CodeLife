# Xgboost

Xgboost是gbdt算法的一种实现框架，但在算法细节上与原始gbdt也所有差异

理解原理需要先理解[GBDT](./0019-GBDT.md)与[CART](./0017-CART%E7%AE%97%E6%B3%95.md)算法。

## 目标

与GBDT一样，Xgboost同样使用前向分步法求解加性模型
$$
\hat y_i = \sum_{k=1}^K f_k(x_i) \\
\hat y_{i}^t = \hat y_{i}^{t-1} + f_t(x_i)
$$
不一样的是在求解每一步模型时就考虑模型正则化
$$
\begin{aligned}
    Obj^t &= \sum_{i=1}^{N} l(y_i, \hat y_i^{t}) + \sum_{i=1}^{t} \Omega(f_i) \\
        &= \sum_{i=1}^{N} l \left(y_i, \hat y_{i}^{t-1} + f_t(x_i) \right) + \Omega(f_t) + const
\end{aligned}
$$

使用泰勒展开对损失函数进行近似
![](img/0020-1.png)

**解释**：将$l(y_i, \hat y_{i}^{t-1})$看成$f(x)$，如果样本加权，将$a_i l(y_i, \hat y_{i}^{t-1})$看成$f(x)$，$f_t(x_i)$看成$\Delta x$。与GBDT一样，在梯度的负方向上，损失较小最快。只不过GBDT用的是一阶梯度，Xgboost里用了二阶梯度。

![](img/0020-2.png)


## 正则化

可以有多种正则化条件，如
1. 叶子节点的个数，防止过拟合
2. 树的深度，防止过拟合
3. 叶子节点输出的L2范数，让输出更平滑

如果以1和3为正则化项

![](img/0020-3.png)

## 求解

对目标函数进行化简，可以得到如下结果，其中$w_{q(x_i)}$表示样本$x_i$属于的$q(x_i)$叶子节点的输出值，一个叶子节点内的输出值一样。

![](img/0020-4.png)

这是T个一元二次方程方程，有闭式解
![](img/0020-5.png)


## 模型构建

1. 开始第t次迭代
2. 对每个样本计算$g_i$,$h_i$
3. 自定向下构建树：
   1. 起始只有一个根节点，包含所有样本
   2. 遍历每个特征，遍历每个分割点（对样本以特征进行排序，从左往右扫描），计算
    ![](img/0020-6.png)
   3. 取使Gain最大的分割点
4. 模型更新，每次模型更新会乘上一个系数$\epsilon$,通常设为0.1，类似于学习率。表示每次不会步子跨太大，给后面模型机会，防止过拟合。
   $$
    \hat y_{i}^t = \hat y_{i}^{t-1} + \epsilon f_t(x_i)
   $$

总体时间复杂度O(M n d k log n)，M次迭代，d个特征，每个特征nlogn排序（计算Gain最大O(n)可忽略），k层需要做k次。

对于离散特征，使用one-hot编码代替原有特征，应用以上方法处理即可。

如果Gain太小，甚至小于可以停止构建。或者构建完之后，再将Gain小于0的节点剪枝掉。

## 工程优化


## Xgboost与GBDT算法上的区别

1. 目标函数不一样，Xgboost考虑了正则化，GBDT只有经验损失。
2. Xgboost进行了二阶求导，误差下降更快。GBDT只进行了一阶梯度。
3. Xgboost可以支持其他基分类器，GBDT是CART树。
4. Xgboost加了Shrinkage，每次更新会乘上一个系数
5. Xgboost会进行列采样，参见[随机森林](./0021-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.md)


