# 分类决策树

一棵决策树的内部节点表示一个特征或属性的，每一条路径是一条测试规则，叶子节点是经过一系列测试之后得到的类别。

决策树的生成有三步：
1. 特征选择：选择最优特征，根据该特征对数据进行分割，使得对各个子数据集有一个最好的分类过程
2. 决策树生成
3. 决策树剪枝：通过1、2步骤生成的决策树虽然对数据有比较好的拟合能力，但是很可能过拟合，通过剪枝提高泛化能力


## 特征选择

特征选取在于选取对训练数据具有分类能力的特征，，信息增益和信息增益比是两种用来衡量特征分类能力的准则。

### 信息增益

随机变量X服从分布:
$$
P(X=x_i) = p_i，i=1,2,3,...,n
$$

信息熵定义：
$$
H(X) = -\sum_{i=1}^{n}p_i \log p_i
$$

当$p_i=0,0\log 0 = 0$，通常取2为底或自然指数e为底，以2为底的时候，熵可以理解为存储一个数据需要的期望比特。

信息熵表达的是随机变量的不确定性，熵越大，随机变量的不确定性越大

条件熵：
$$
H(Y|X) = \sum_{i=1}^{n} p_i H(Y|X=x_i)
$$

特征A对数据集D的信息增益定义如下，即知道特征A后，数据集分类不确定性的减少量
$$
gain(D, A) = H(D)-H(D|A) \\
H(D) = -\sum_{k=1}^{K}\frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D_|} \\
H(D|A) = \sum_{i=1}^{n} \frac{|D_i|}{|D|}H(D_i) = -\sum_{i=1}^{n} \frac{|D_i|}{|D|} \sum_{k=1}^{K} \frac{|D_{ik}|}{|D_i|} \log_2 \frac{|D_{ik}|}{|D_i|}
$$

其中：$|C_k|$表示类别$C_k$的样本数量，$|D_i|$表示第i个子集的样本数量，$|D_{ik}|$表示第i个子集中类比$C_k$的样本数量。


### 信息增益比

以信息增益作为划分准则会偏向于选择取值较多的特征，使用信息增益比可以矫正。

$$
gain_r(D, A) = \frac{gain(D,A)}{H_A(D)} \\
H_A(D) = -\sum_{i=1}^{n} \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}
$$

其中$H_A(D)$为数据集D关于特征A分布的熵，n为特征A的特征值个数

### 注意点

1. 连续特征处理：对于连续特征，将特征值进行排序，假设有n个，每两个特征值之间可以取中点作为划分点。
2. 缺失值处理：按已存在属性值的比例对缺失值样本进行划分

## 决策树生成

### ID3

输入：训练集D，特征集A，阈值e
输出：决策树
步骤：
1. 如果如果D中所有实例都属于同一类$C_k$，生成单节点树，并将$C_k$作为类别标记
2. 如果A为空集，生成单节点树，并将最大类别$C_k$作为类别标记
3. 计算A中各特征的信息增益，选择最大的特征Ag
4. 如果Ag的信息增益小于e，不做划分，返回单节点树，并将最大类别$C_k$作为类别标记
5. 否则，对Ag的每个可能值$a_i$，依$A_g = a_i$将D划分为若干子集，将$D_i$中最大类别$C_k$作为类别标记，并构建子节点
6. 对于第i个子节点，以$D_i$为训练集，$A-A_g$为特征集，递归调用1-5

### C4.5

将ID3中第三步换成计算信息增益比即可。

## 决策树剪枝

决策树生成是贪心的做法，考虑的局部最优。而决策树剪枝是需要考虑全局最优。

定义以下损失函数：
$$
C_{\alpha}(T) = \sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|
$$
其中$|T|$表示叶子节点的个数，$t$表示其中一个叶子节点，$N_t$表示叶子节点的样本数量，$H_t(T)$表示叶子节点的熵
$$
H_t(T) = -\sum_{k}\frac{N_{tk}}{N_t} \log \frac{N_{tk}}{N_t}, k = 1,2,...,K
$$

从叶子节点出发，如果向上合并到父节点能使损失函数减小，则进行剪枝，具体算法如下：

输入：树T，参数$\alpha$
输出：修剪后的子树$T_{\alpha}$
步骤：
1. 计算每个节点的熵
2. 递归得从树的叶子节点向上缩
3. 如果一组叶子节点回缩前与回缩后的树分别是$T_A、T_B$,并且$C_{\alpha}(T_A) \le C_{\alpha}(T_B)$，则进行回缩
4. 返回步骤2直到不能继续

其实就是一个树状dp