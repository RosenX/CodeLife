# GBDT

GBDT(Gradient Boosting Decision Tree)是一种有以分类树和回归树为基模型的boosting方法

## 基本提升树

提升树模型可以表达为加法模型
$$
f_M(x) = \sum_{m=1}^M T(x;\Theta_m)
$$

采用前向分步法进行优化，每一步优化一个模型：
$$
\hat \Theta_m = \arg \min_{\Theta_m} \sum_{i=1}^N L(y_i, f_{m-1}(x_i)+T(x_i; \Theta_m))
$$

### 分类问题

参见[AdaBoost](./0018-AdaBoost.md)，只要将基分类器限定为二分类树，损失函数限定为指数函数即可。

### 回归问题

使用CART树，并且使用均方误差作为损失函数。对于第m棵树，
$$
\begin{aligned}
    Loss &= \sum_{i=1}^N L(y_i, f_{m-1}(x_i)+T(x_i; \Theta_m)) \\
&= \sum_{i=1}^N (y-f_{m-1}(x)-T(xi;\Theta_m))^2 \\
&= \sum_{i=1}^N (r_{mi}-T(x_i; \Theta_m))^2
\end{aligned}
$$

也就是说，对于第m棵树，只需要拟合每个样本的残差即可。

### 梯度提升树

对于优化目标，如果损失函数是指数函数，或者是均方误差，每一步优化很简单。但是对于一般的损失函数，优化并不容易

$$
\hat \Theta_m = \arg \min_{\Theta_m} \sum_{i=1}^N L(y_i, f_{m-1}(x_i)+T(x_i; \Theta_m))
$$

考虑一般情况，对于损失函数$J(\theta)$，为了让$J(\theta)$最小，每次沿着负梯度方向下降即可。将$f_{m-1}(x_i)$看做$\theta$，怎样才能让$L$下降最快呢？只需要
$$
T(x_i; \Theta_{m}) = -\frac{\partial L(y_i, f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}
$$

当损失函数为均方误差时，残差$r_{mi} = y-f_{m-1}(x_i)$是上式的特殊情况：
$$
-\frac{\partial(y-f_{m-1}(x_i))^2}{\partial f_{m-1}(x_i)} = -2\times(y-f_{m-1}(x_i))\times -1 \varpropto  (y-f_{m-1}(x_i))
$$

## 参考

1. 李航《机器学习方法》
2. https://zhuanlan.zhihu.com/p/139381538




