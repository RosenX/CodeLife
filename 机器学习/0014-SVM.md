# SVM

- [SVM](#svm)
  - [函数间隔与几何间隔](#函数间隔与几何间隔)
  - [线性可分支持向量机-硬间隔](#线性可分支持向量机-硬间隔)
    - [目标](#目标)
    - [决策函数](#决策函数)
    - [支持向量](#支持向量)
  - [线性支持向量机-软间隔](#线性支持向量机-软间隔)
    - [目标](#目标-1)
    - [决策函数](#决策函数-1)
  - [对偶求解](#对偶求解)
  - [非线性支持向量机](#非线性支持向量机)
    - [核方法](#核方法)
    - [目标](#目标-2)
    - [决策函数](#决策函数-2)
  - [序列最小最优化算法](#序列最小最优化算法)
    - [SMO算法](#smo算法)
  - [参考](#参考)

SVM是一种广义线性二分类器，SVM是希望在特征空间（可能是原始数据空间，也可能是映射之后的高维特征空间）中找到一个超平面。如果数据可分，超平面与数据间隔最大，如果数据不可分，误差最小。

## 函数间隔与几何间隔

符号：超平面$wx+b=0$，样本集(X,Y)

函数间隔：超平面在样本集上的函数间隔为所有样本点与超平面函数间隔的最小值
$$
\hat{\gamma_i} = y_i(w*x_i+b) \\
\hat{\gamma} = \min \hat{\gamma_i}
$$

几何间隔：对超平面法向量进行限制，使得函数间隔不会随w、b同比例变化而变化

$$
\gamma_i = y_i(\frac{w}{||w||}*x_i+\frac{b}{||w||}) \\
\gamma = \min \gamma_i
$$

## 线性可分支持向量机-硬间隔

### 目标

如果数据线性可分，线性可分支持向量机希望找到一个超平面，使得几何间隔最大，优化目标为：
$$
\begin{aligned}
\max_{w,b}& \,\,\, \gamma  \\
s.t. & \,\,\, y_i(\frac{w}{||w||}x_i+\frac{b}{||w||}) \ge \gamma , i \in [1,N]
\end{aligned}
$$
因为函数间隔与几何间隔关系，$\gamma = \frac{\hat{\gamma}}{||w||}$，优化目标可转化为：
$$
\begin{aligned}
\max_{w,b}& \,\,\, \frac{\hat{\gamma}}{||w||}  \\
s.t. & \,\,\, y_i(wx_i+b) \ge \hat{\gamma} , i \in [1,N]
\end{aligned}
$$
注意到，优化目标其实与$\hat{\gamma}$没有关系，因为同比例放大$w,b$，$\frac{\hat{\gamma}}{||w||}$是不变的，因此可以令$\hat{\gamma}=1$。

而且，最大化$\frac{1}{||w||}$与最小化$\frac{||w||^2}{2}$是等价的，优化目标可改写为：
$$
\begin{aligned}
\min_{w,b}& \,\,\, \frac{1}{2}||w||^2  \\
s.t. & \,\,\, y_i(wx_i+b) -1 \ge 0 , i \in [1,N]
\end{aligned}
$$
之所以这么转化，是为了将问题转化为凸二次规划为题，方便后面求解。

### 决策函数

可以解出超平面
$$
w^{*}x+b^{*} =0
$$
决策函数是
$$
f(x) = sign(w^{*}x+b^{*})
$$

$f(x)>0$是正类，小于0是负类。

### 支持向量

如果数据可分，$wx+b=0$是几何间隔最大的超平面，$wx+b=1$和$wx+b=-1$分别是间隔为1和间隔为-1的边界，决策边界其实由这两条边界上的点即可确定，这两条边界上的点也叫做支持向量。


## 线性支持向量机-软间隔

### 目标
线性可分支持向量机对于数据可分的假设太理想化，实际情况大多不可分，即函数间隔不一定大于等于1。在这种情况下，可以允许每个样本间隔存在误差，最终目标是误差最小即可。这种间隔也叫软间隔。

$$
\begin{aligned}
\min_{w,b}& \,\,\, \frac{1}{2}||w||^2+C \sum_{i=1}^N \epsilon_i  \\
s.t. & \,\,\, y_i(wx_i+b) \ge 1-\epsilon_i , i \in [1,N] \\
s.t. & \,\,\, \epsilon_i \ge 0, i \in [1, N]
\end{aligned}
$$

这个目标其实等同于合页损失：
$$
\sum_{i=1}^{N}max(1-y_i(wx_i+b), 0)+\lambda||w||^2
$$

令$max(1-y_i(wx_i+b), 0)=\epsilon_i，\lambda = \frac{1}{2C}$即可得到原始目标

### 决策函数

同线性可分支持向量机

## 对偶求解

支持向量机的原目标二次规划问题，求解复杂度较高。为了降低复杂度，同时也是为了方便引入核方法，使用对偶方法进行求解。是对偶方法参见[笔记](../数学/0004-拉格朗日及对偶问题.md)

以软间隔线性支持向量机求解为例，拉格朗日函数为：
$$
L(w,b,\epsilon,\alpha,\mu) = \frac{1}{2}||w||^2+C\sum_{i=1}^{N}\epsilon_i-\sum_{i=1}^N\alpha_i[y_i(wx_i+b)-1+\epsilon_i]-\sum_{i}^{N}\mu_i\epsilon_i
$$
其中$\alpha_i \ge 0, \mu_i \ge 0$，对偶问题是：
$$
\max_{\alpha, \mu} \min_{w,b,\epsilon} L(w,b,\epsilon,\alpha,\mu)
$$

首先求$\min_{w,b,\epsilon} L(w,b,\epsilon,\alpha,\mu)$,用拉格朗日函数分别对$w,b,\epsilon$求导，
$$
\nabla_w L = w-\sum_{i=1}^N\alpha_iy_ix_i = 0 \\
\nabla_b L = -\sum_{i=1}^N \alpha_i y_i = 0 \\
\nabla_{\epsilon_i} L = C-\alpha_i-\mu_i = 0
$$
代入$L(w,b,\epsilon,\alpha,\mu)$中得到
$$
\min_{w,b,\epsilon} L = -\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^{N}\alpha_i \alpha_j y_i y_j(x_ix_j)+\sum_{i=1}^{N} \alpha_i
$$
再对$\min_{w,b,\epsilon} L$求极大：
$$
\begin{aligned}
    \max_{\alpha} &\,\,\,  -\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^{N}\alpha_i \alpha_j y_i y_j(x_ix_j)+\sum_{i=1}^{N} \alpha_i \\
    s.t. & \,\,\, \sum_{i=1}^{N}\alpha_i y_i = 0 \\
    & \,\,\,  0 \le \alpha_i \le C \\
\end{aligned}
$$
其中$0 \le \alpha_i \le C$由$C-\alpha_i-\mu_i=0, \alpha_i \ge 0, \mu_i \ge 0$融合得到。

假设$\alpha^{*}$是对偶问题的解，且其中一个分量$0 \le \alpha_j \le C$，那么原问题的解是：
$$
w^* = \sum_{i=1}^{N}\alpha_i^{*}y_ix_i \\
b^* = y_j - \sum_{i=1}^{N}y_i \alpha^{*}x_ix_j
$$
其中$w^*$由$\nabla_wL=0$得到，$b^*$由KKT条件:
$$
\alpha_j(y_j(w^*x_j+b)-1+\epsilon_j) = 0 \\
\mu_j \epsilon_j = 0 \\
0 < \alpha_j < C \\
\mu_i = C-\alpha_j > 0 \\

\Downarrow \\

\epsilon_j = 0 \\
y_j(w^*x_j+b)-1 = 0
$$
求得

## 非线性支持向量机

线性支持向量机能解决线性分类问题，对于非线性分类问题不适用。

### 核方法

对与非线性问题，核方法的思想上将数据映射到高维特征空间，然后再寻找超平面使数据可分。

但核方法不会直接定义原始空间与特征空间的映射函数$\phi(x):X \rightarrow H$，因为这个映射函数往往很难确定，而是定义核函数:

> 对所有$x, z \in$ X, 核函数满足条件 $K(x,z) = \phi(x) \cdot \phi(z)$，$\cdot$表示点积

常用核函数：
1. 多项式核函数：$K(x,z) = (x\cdot z+1)^p$
2. 高斯核函数：$K(x,z) = exp(-\frac{||x-z||^2}{2\sigma^2})$

不是所有函数都可以作为核函数，需要满足一定条件，具体可查阅相关资料。

### 目标

软间隔支持向量机对偶问题中有$x_i x_j$项，正好符合核函数的形式，因此核方法支持向量机有以下目标：
$$
\begin{aligned}
    \min_{\alpha} &\,\,\, \frac{1}{2}\sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j K(x_i, x_j) - \sum_{i=1}^N\alpha_i \\
    s.t. &\,\,\, \sum_{i=1}^N \alpha_i y_i = 0 \\
        & \,\,\, 0 \le \alpha_i \le C
\end{aligned}
$$

同软间隔求解方法得：
$$
w^* = \sum_{i=1}^{N}\alpha_i^{*}y_ix_i \\
b^* = y_j - \sum_{i=1}^{N}y_i \alpha^{*}K(x_i, x_j)
$$

### 决策函数

将$w^*,b^*$代入$f(x) = sign(w^*x+b)$即可得到


## 序列最小最优化算法

以上只是推导了只需要求解$\alpha$即可得到问题的解，如何求解$\alpha$还需要解决。序列最小最优化(SMO)算法是其中一种解法。

要解决的问题：
$$
\begin{aligned}
    \min_{\alpha} &\,\,\,  \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^{N}\alpha_i \alpha_j y_i y_j(x_ix_j)-\sum_{i=1}^{N} \alpha_i \\
    s.t. & \,\,\, \sum_{i=1}^{N}\alpha_i y_i = 0 \\
    & \,\,\,  0 \le \alpha_i \le C \\
\end{aligned}
$$

### SMO算法

思想：如果所有拉格朗日乘子变量的解都满足KKT条件，那么最优化问题就解决了。采用启发式方法，每次选择两个变量，固定其他变量，针对这两个变量可以构造一个二次规划问题，有解析解。SMO算法分为两部分：1）启发式选择两个变量；2）求解析解。

具体参见：《机器学习方法》李航P126

## 参考

《机器学习方法》李航
