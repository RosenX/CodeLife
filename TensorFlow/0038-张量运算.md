# 张量运算

## 标量运算

加减乘除乘方，以及三角函数，指数，对数等常见函数，逻辑比较运算符等都是标量运算符。标量运算符的特点是对张量实施逐元素运算。

有些标量运算符对常用的数学运算符进行了重载。并且支持类似numpy的广播特性。

```python
a = tf.constant([[1.0,2],[-3,4.0]])
b = tf.constant([[5.0,6],[7.0,8.0]])

# 算术运算符
a+b
a-b
a*b
a/b
a**2
a**(0.5)
a%3
a//3

# 比较运算符
a>=2
(a>=2)&(a<=3)
(a>=2)|(a<=3)
a==5

# 函数
tf.sqrt(a)

a = tf.constant([1.0,8.0])
b = tf.constant([5.0,6.0])
c = tf.constant([6.0,7.0])
tf.add_n([a,b,c]) # 所有元素相加 => (12, 21)

tf.print(tf.maximum(a,b)) # 对应位置元素比较
tf.print(tf.minimum(a,b))
```

## 向量运算

向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。 许多向量运算符都以reduce开头。

```python
a = tf.range(1,10)
tf.print(tf.reduce_sum(a))
tf.print(tf.reduce_mean(a))
tf.print(tf.reduce_max(a))
tf.print(tf.reduce_min(a))
tf.print(tf.reduce_prod(a))

b = tf.reshape(a,(3,3))
# axis指定在哪一轴运算，运算完之后那一轴会变成1
# keepdims=true, 原来是多少维，运算完之后还是多少维，否则会减一维
tf.print(tf.reduce_sum(b, axis=1, keepdims=True)) 
tf.print(tf.reduce_sum(b, axis=0, keepdims=True))

#bool类型的reduce
p = tf.constant([True,False,False])
q = tf.constant([False,False,True])
tf.print(tf.reduce_all(p)) #=> 0
tf.print(tf.reduce_any(q)) #=> 1

# tf.foldr：从最后一个元素往前做reduce操作
s = tf.foldr(lambda a,b:a-b,tf.reshape(tf.range(1,10), (3,3)))
tf.print(s)

# 累加、累乘数组
a = tf.range(1,10)
tf.print(tf.math.cumsum(a)) # 1,1+2,1+2+3,...
tf.print(tf.math.cumprod(a)) # 1,1*2,1*2*3,...

#arg最大最小值索引
a = tf.range(1,10)
tf.print(tf.argmax(a))
tf.print(tf.argmin(a))

#tf.math.top_k可以用于对张量排序
a = tf.constant([1,3,7,5,4,8])
values,indices = tf.math.top_k(a,3,sorted=True) # 输出结果排序 
tf.print(values)
tf.print(indices)
```

## 矩阵运算

矩阵运算包括：矩阵乘法，矩阵转置，矩阵逆，矩阵求迹，矩阵范数，矩阵行列式，矩阵求特征值，矩阵分解等运算。

除了一些常用的运算外，大部分和矩阵有关的运算都在tf.linalg子包中。

```python
#矩阵乘法
a = tf.constant([[1,2],[3,4]])
b = tf.constant([[2,0],[0,2]])
a@b  #等价于tf.matmul(a,b)

#矩阵转置
a = tf.constant([[1.0,2],[3,4]])
tf.transpose(a)

#矩阵逆，必须为tf.float32或tf.double类型
a = tf.constant([[1.0,2],[3.0,4]],dtype = tf.float32)
tf.linalg.inv(a)

#矩阵求trace
tf.linalg.trace(a)

#矩阵求范数
tf.linalg.norm(a)

#矩阵行列式
tf.linalg.det(a)

#矩阵特征值
tf.linalg.eigvalsh(a)

# QR分解
q,r = tf.linalg.qr(a)
tf.print(q)
tf.print(r)
tf.print(q@r)

#svd分解
v,s,d = tf.linalg.svd(a)
tf.matmul(tf.matmul(s,tf.linalg.diag(v)),d)
```

## 广播

TensorFlow的广播规则和numpy是一样的:

1. 如果张量的维度不同，将维度较小的张量进行扩展，直到两个张量的维度都一样。
2. 如果两个张量在某个维度上的长度是相同的，或者其中一个张量在该维度上的长度为1，那么我们就说这两个张量在该维度上是相容的。
3. 如果两个张量在所有维度上都是相容的，它们就能使用广播。
4. 广播之后，每个维度的长度将取两个张量在该维度长度的较大值。
5. 在任何一个维度上，如果一个张量的长度为1，另一个张量长度大于1，那么在该维度上，就好像是对第一个张量进行了复制。
   
tf.broadcast_to 以显式的方式按照广播机制扩展张量的维度。


```python
a = tf.constant([1,2,3])
b = tf.constant([[0,0,0],[1,1,1],[2,2,2]])
b + a  #等价于 b + tf.broadcast_to(a,b.shape)

c = tf.constant([1,2,3])
d = tf.constant([[1],[2],[3]])
c+d # 等价于 tf.broadcast_to(c,[3,3]) + tf.broadcast_to(d,[3,3])

#计算广播后计算结果的形状，静态形状，TensorShape类型参数
tf.broadcast_static_shape(a.shape,b.shape)
```