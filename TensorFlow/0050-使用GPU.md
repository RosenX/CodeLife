# 使用单GPU
无论是内置fit方法，还是自定义训练循环，从CPU切换成单GPU训练模型都是非常方便的，无需更改任何代码。当存在可用的GPU时，如果不特意指定device，tensorflow会自动优先选择使用GPU来创建张量和执行张量计算。

但tensorflow会默认获取全部GPU的全部内存资源权限，需要限制

```python
gpus = tf.config.list_physical_devices("GPU")

if gpus:
    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU
    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用
    # 或者也可以设置GPU显存为固定使用量(例如：4G)
    #tf.config.experimental.set_virtual_device_configuration(gpu0,
    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) 
    tf.config.set_visible_devices([gpu0],"GPU") 
```

# 使用多GPU

## GPU拆分

如果是单GPU，可以将一个GPU拆成多个
```python
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  # 设置两个逻辑GPU模拟多GPU训练
  try:
    tf.config.experimental.set_virtual_device_configuration(gpus[0],
        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),
         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)]) # 拆
    logical_gpus = tf.config.experimental.list_logical_devices('GPU')
    print(len(gpus), "Physical GPU,", len(logical_gpus), "Logical GPUs")
  except RuntimeError as e:
    print(e)
```

## 多GPU同步

MirroredStrategy过程简介：

1. 训练开始前，该策略在所有 N 个计算设备上均各复制一份完整的模型；
2. 每次训练传入一个批次的数据时，将数据分成 N 份，分别传入 N 个计算设备（即数据并行）；
3. N 个计算设备使用本地变量（镜像变量）分别计算自己所获得的部分数据的梯度；
4. 使用分布式计算的 All-reduce 操作，在计算设备间高效交换梯度数据并进行求和，使得最终每个设备都有了所有设备的梯度之和；
5. 使用梯度求和的结果更新本地变量（镜像变量）；
6. 当所有设备均更新本地变量后，进行下一轮训练（即该并行策略是同步的）。
```python
%%time
strategy = tf.distribute.MirroredStrategy()  
with strategy.scope():
    model = create_model()
    model.summary()
    model = compile_model(model)

history = model.fit(ds_train,validation_data = ds_test,epochs = 10)
```